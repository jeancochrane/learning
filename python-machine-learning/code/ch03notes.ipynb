{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: A Tour of Machine Learning Classifiers\n",
    "\n",
    "### General principles\n",
    "\n",
    "Selecting a classifier is a difficult but critical task in machine learning! For any given task, it's good practice to **compare different models** and test them for performance, since no single classifier is universally the \"best.\" All involve costs and benefits for any application.\n",
    "\n",
    "Some common features of a dataset to consider when choosing models:\n",
    "\n",
    "- How **noisy** is the dataset?\n",
    "- Are the classes **linearly separable** (as is necessary for perceptron and Adaline, for example)?\n",
    "- How many **dimensions** are you working with? (How \"big\" is your data?)\n",
    "\n",
    "### Five steps to train any algorithm\n",
    "\n",
    "1. Select **features** (and optionally perform *feature scaling*)\n",
    "2. Choose a **performance metric** (*cost function*) so you know how well your model is doing\n",
    "3. Choose one or more **classifiers** (Adaline? Perceptron?) and **optimization algorithms** (batch descent? SGD?)\n",
    "4. **Evaluate** the performance of your model(s)\n",
    "5. Tune your **hyperparameters** (epoch, learning rate)\n",
    "\n",
    "## New classifiers\n",
    "\n",
    "### Logistic regresion\n",
    "\n",
    "Logistic regression is a classification algorithm very closely related to Adaline and the perceptron. It's also a **linear model for binary classification**, and it follows the same neural architecture:\n",
    "\n",
    "```\n",
    "Input  -\\  <------------------------- error: update weights            1\n",
    "Weight --\\                                               |            /\n",
    "Input -----> net input function --> activation function -|-> quantizer\n",
    "Weight --/                                                            \\ \n",
    "Input  -/                                                              -1\n",
    "```\n",
    "\n",
    "In a logistic regression model, however, the **activation function** and **cost function** are very different from the two learning algorithms we've studied! Changing these two functions, as we'll see, will have the nice effect of allowing us to make good classifications even when the data is *not* perfectly linearly separable.\n",
    "\n",
    "#### Activation: the logistic (*sigmoid*) function\n",
    "\n",
    "The activation function in logistic regression is (shockingly!) called the **logistic function,** or sometimes the \"sigmoid function\" in reference to its curvy shape. The intuition behind the logistic function is that it takes inputs as any number on the real line, and squishes them onto the interval $[0, 1]$. When performed correctly, this has the really cool property of returning the **confidence** of a classification by allowing us to interpret the output of the function as a probability of the class label $\\hat{y_{i}} = 1$.\n",
    "\n",
    "To derive the logistic function, let's start by defining the **odds** function, that takes the probability, $p$, of an event, and returns a proportion of how much more likely that event is to happen ($p$) than to not happen ($1-p$):\n",
    "\n",
    "$$odds = \\frac{p}{(1-p)}$$\n",
    "\n",
    "where in our case, $p$ represents the probability that $\\hat{y_{i}} = 1$.\n",
    "\n",
    "One way of mapping this probability to the real line is the **logit function**, which is the natural log of the odds:\n",
    "\n",
    "$$logit(p) = log(odds) = log(\\frac{p}{1-p})$$\n",
    "\n",
    "But for the purposes of classification, we're more interested in the *inverse* of the logit function, the **logistic function**, which will map numbers from the real line to probabilities:\n",
    "\n",
    "$$logistic(z) = \\phi(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Here, as in the previous learning algorithms we've studied, $z$ is a linear combination of the weight vector and the input vector, $w^{T}z$. As such, we can interpret the logistic function as taking in the net input function and returning the probability that the sample has a class label of 1, given $x$ weighted by $w$:\n",
    "\n",
    "$$\\phi(z_{i}) = P(y_{i}=1 \\mid x ; w)$$\n",
    "\n",
    "#### Quantizer: same as it ever was\n",
    "\n",
    "On first glance, the quantizer function for logistic regression looks different from the quantizer functions we saw in chapter 2:\n",
    "\n",
    "$$\\hat{y} = \\bigg\\{ \\begin{array}{ll} 1 & if \\> \\phi(z) \\ge 0.5 \\\\ 0 & otherwise \\end{array}$$\n",
    "\n",
    "Immediately, two characteristics of this function jump out as different:\n",
    "\n",
    "1. The positive class label requires that $\\phi(z)$ be greater than $0.5$, not $0$\n",
    "2. The possible class labels are $[0, 1]$, not $[-1, -1]$\n",
    "\n",
    "Point #2 is indeed different, but makes little difference: we define the negative class to be $0$ because it better fits the properties of the logistic function, not for any deep purpose. \n",
    "\n",
    "But on closer inspection, point #1 is revealed to be a red herring! Since $\\phi(0) = 0.5$ and $\\phi(z)$ is *monotonically increasing*, we can prove that\n",
    "\n",
    "$$\\hat{y} = \\bigg\\{ \\begin{array}{ll} 1 & if \\> z \\ge 0 \\\\ 0 & otherwise \\end{array}$$\n",
    "\n",
    "Which is identical to the quantizer used in Adaline/perceptron learning, just with a new negative class label!\n",
    "\n",
    "#### Cost function: brand new territory\n",
    "\n",
    "Since logistic regresion deals with odds, our goal in defining a cost function will be to **maximize the likelihood** of a correct classification. We'll achieve this by defining likelihood of correct classification, by taking the *natural log* of that likelihood (which will be easier to differentiate), and then inverting the signs to instead minimize the likelihood of an *incorrect* classification.\n",
    "\n",
    "Let's start by defining the likelihood of a correct classification, $L(w)$:\n",
    "\n",
    "$$L(w) = P(y \\mid x ; w)$$\n",
    "\n",
    "By assuming that our samples are *independent*, we can reason that this is equivalent to taking the cumulative product across samples:\n",
    "\n",
    "$$L(w) = \\prod_{i=1}^{n} P \\left( y^{(i)} \\mid x^{(i)} ; w \\right)$$\n",
    "\n",
    "And finally, we'll sub in our activation function to define the probabilities in terms of the net input $z$:\n",
    "\n",
    "$$L(z^{(i)}) = \\prod_{i=1}^{n} \\left( \\phi(z^{(i)} \\right)^{y^{(i)}} \\left( 1 - \\phi(z^{(i)}) \\right)^{(1 - y^{(i)})}$$\n",
    "\n",
    "Notice that the \"incorrect\" term cancels based on the value of the class label $y^{(i)}$. When the class label is positive ($1$), we return the positive probability $\\phi(z^{(i)}$ for the sample, since the other term evaluates to $1$; when the class label is negative ($0$), we return the negative probability $(1 - \\phi(z^{(i)}))$. Mathematically, we get the following stepwise function for the likelihood:\n",
    "\n",
    "$$L(z) = \\bigg\\{ \\begin{array}{ll} \\phi(z) & if \\> y = 1 \\\\ 1 - \\phi(z) & otherwise \\end{array}$$\n",
    "\n",
    "It's hard to maximize over products, however, so instead of using the likelihood, we'll use the **log-likelihood**, $l(w) = log(L(w))$. (The log-likelihood also helps prevent **numeric underflow** when we have extremely small likelihoods.) Using properties of logs, we can simplify $log(L(w))$ like so:\n",
    "\n",
    "$$l(w) = \\sum_{i=1}^{n} \\bigg [ y^{(i)}log(\\phi(z^{(i)})) + (1 - y^{(i)})log( 1 - \\phi(z^{(i)})) \\bigg ]$$\n",
    "\n",
    "But since minimization is easier, we'll actually want to minimize the negative log-likelihood – our final cost function $J$!\n",
    "\n",
    "$$J(w) = -l(w) = \\sum_{i=1}^{n} \\bigg [ -y^{(i)}log(\\phi(z^{(i)})) - (1 - y^{(i)})log( 1 - \\phi(z^{(i)})) \\bigg ]$$\n",
    "\n",
    "#### Overfitting and underfitting (bias and variance)\n",
    "\n",
    "**\"Overfitting\"** is a term we can use to describe a model that fits the training data \"too well,\" such that it doesn't generalize well to new samples. (The opposite of overfitting is **underfitting**, where a model is not complex enough – usually meaning that it is not a high enough degree polynomial – to capture the underlying process that produces the data.)\n",
    "\n",
    "Another way of describing problems with a model due to overfitting/underfitting is to describe the model as high variance (overfit) or high bias (underfit). Both terms describe *consistency*:\n",
    "\n",
    "- A **high variance** model has *inconsistent predictions*, meaning that when we retrain the model on different subsets of the training data we get different predictions, since our model is fitting itself to the noise in the data and not the underlying phenomenon. (While \"variance\" is a summary statistic often used to describe a dataset, in this case we mean to describe the wide range of different predictions that the model produces when trained on different subsets.)\n",
    "- A **high bias** model has *consistent residuals*, meaning that when we retrain the model on different subsets of training data we get errors (or costs) that follow a pattern. As in the case of a \"biased estimator,\" residuals following their own pattern are an indication that our errors are not due to the randomness of our training data, but rather due to our classifier's inability to model the true phenomenon underlying the data.\n",
    "\n",
    "Bias and variance are often seen as sources of error that need to be *balanced* in the training of a model. Hence, we use the phrase **bias/variance tradeoff** to describe the way that decreasing bias can often increase variance, and vice versa. For an in-depth explanation of the bias/variance tradeoff (with good visual aids to boot), see [Scott Fortmann-Roe's excellent breakdown of the topic.](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "#### L2 Regularization\n",
    "\n",
    "When training classifiers, **regularization** can be a useful method for finding a good balance between bias and variance. Raschka covers **L2 regularization**, a specific algorithm that *adds* noise (bias) to a model, in the form of a proportional squared term, to penalize extreme weights while learning.\n",
    "\n",
    "Under L2 Regularization, we'll tweak our cost function like so:\n",
    "\n",
    "$$ J(w) := J(w) + \\frac{\\lambda}{2} \\mid \\mid x \\mid \\mid^{2} $$\n",
    "\n",
    "Where $\\mid \\mid x \\mid \\mid$ is the length of the weight vector, $\\sqrt{\\sum_{i=1}^{n} w_{j}^2}$, and $\\lambda$ is the **regularization parameter** that controls the *strength* of the regularization. The stronger the regularization parameter (where $\\lambda$ is large), the more drastic the update will be when the values of the weight vector $w$ are large. (Note that L2 regularization requires scaled features, since the weights need to all be on the same scale for the cost function to accurately model an increase in bias.)\n",
    "\n",
    "By convenction, we often use the regularization parameter $C$, the inverse of $\\lambda$. The relationship between the two parameters is straightforward:\n",
    "\n",
    "$$ C = \\frac{1}{\\lambda}$$\n",
    "\n",
    "Multiplying through the above cost function by $C$ gives us a slightly altered version:\n",
    "\n",
    "$$ J(w) := C \\bigg [ J(w) \\bigg ] + \\frac{1}{2} \\mid \\mid x \\mid \\mid^{2}$$ \n",
    "\n",
    "which expands out to\n",
    "\n",
    "$$ J(w) := C \\bigg [ \\sum_{i=1}^{n} -y^{(i)}log(\\phi(z^{(i)})) - (1 - y^{(i)})log(1 - \\phi(z^{(i)})) \\bigg ] + \\frac{1}{2} \\mid \\mid x \\mid \\mid^{2}$$\n",
    "\n",
    "Since $C$ is the inverse of $\\lambda$, large values of $C$ correspond to **low strength** regularization, while small values of $C$ correspond to **high strength** regularization.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "The **Support Vector Machine** (SVM) algorithm is an extension of the perceptron that is very common in classification problems. The major difference is that where the perceptron minimizes misclassification errors, SVM *maximizes the margin* between the decision boundary and the **support vectors**, the samples closest to the decision boundary.\n",
    "\n",
    "### TO DO: fill out information on the algorithm that Raschka doesn't provide\n",
    "\n",
    "#### Slack variables\n",
    "\n",
    "**Slack variables** are an optional addition to the SVM objective function that help improve performance for nonlinearly-separable data. With slack variables, we can allow for specific classification errors in samples that are really close to our decision boundary. We call SVM **soft-margin** classification when we use slack variables, since they allow us to occasionally \"override\" the margin, and **hard-margin** classification otherwise.\n",
    "\n",
    "Recall the linear constraints on the SVM margins:\n",
    "\n",
    "$$ \\bigg\\{ \\begin{array}{ll} w^{T}x^{(i)} \\ge 1 & if \\> y^{(i)} = 1 \\\\ w^{T}x^{(i)} \\le -1 & if \\> y^{(i)} = -1 \\end{array} $$\n",
    "\n",
    "Now, we'll add our slack variables in with signs that depend on the class label, producing a new margin:\n",
    "\n",
    "$$ \\bigg\\{ \\begin{array}{ll} w^{T}x^{(i)} \\ge 1 - \\xi^{(i)} & if \\> y^{(i)} = 1 \\\\ w^{T}x^{(i)} \\le -1 + \\xi^{(i)} & if \\> y^{(i)} = -1 \\end{array} $$\n",
    "\n",
    "This gives us a new objective function to minimize:\n",
    "\n",
    "$$ \\frac{1}{2} \\mid\\mid x \\mid\\mid^{2} + C \\bigg ( \\sum_{i=1}^{n} \\xi^{(i)} \\bigg ) $$\n",
    "\n",
    "Where the parameter $C$ is the **miscclassification penalty** that controls the width of the margin. Note that as $C$ approaches $0$, we approach hard-margin SVM as the effect of the slack variables becomes negligible!\n",
    "\n",
    "#### What are the tradeoffs between SVM and logistic regression?\n",
    "\n",
    "SVM is less sensitive to outliers than logistic regression, since it only considers errors in the points within the margin (whereas logistic regression calculates residuals across the entire dataset).\n",
    "\n",
    "On the other hand, logistic regression is a simpler model, and it's easier to implement. It's also a lot easier to update, which makes it good for online learning (streaming data).\n",
    "\n",
    "### Kernel methods\n",
    "\n",
    "**Kernel methods** describe a set of techniques for seaprating *nonlinearly separable classes* while still using a fundamentally linear model. The **kernel** refers to a similarity function which returns scores that quantify how similar two samples are.\n",
    "\n",
    "### TO DO: revisit this section with better math\n",
    "\n",
    "#### Intution\n",
    "\n",
    "There are three basic steps to any kernel-based learning algorithm:\n",
    "\n",
    "1. Project samples into a **higher-dimensional space** where they *are* linearly separable\n",
    "2. Find the **hyperplane** that separates the samples through a learning algorithm (SVM, logistic regression, etc.)\n",
    "3. Project the samples and hyperplane back into the **original dimension**\n",
    "\n",
    "#### Math\n",
    "\n",
    "In a kernelized algorithm, we'l move dimensions via a **mapping function** $\\phi(\\cdot)$. Raschka gives an example of one possible mapping function that moves from two dimensions to three:\n",
    "\n",
    "$$ \\phi(x_{1}, x_{2}) = (z_{1}, z_{2}, z_{3}) = (x_{1}, x_{2}, x_{1}^{2} + x_{2}^{2}) $$\n",
    "\n",
    "Then, we'll use the inverse function $\\phi^{-1}(\\cdot)$ to move back to two dimensions.\n",
    "\n",
    "Since projection back and forth between dimensions can be expensive when updating weights across an entire dataset, we'll instead define a **kernel function** that measures the distance between samples **(TO DO: why?)** Raschka defines the **Radial Basis Function** (RDF – also known as the Gaussian kernel):\n",
    "\n",
    "$$ k(x^{(i)}, x^{(j)}) = exp \\bigg( -\\gamma \\mid\\mid x^{(i)} - x^{(j)} \\mid\\mid^{2} \\bigg) $$\n",
    "\n",
    "Where $x^{(i)}, x^{(j)}$ are two samples in our dataset, $exp$ sets the interval of the score to $[0, 1]$, $\\mid\\mid x^{(i)} - x^{(j)} \\mid\\mid^{2}$ is the *distance function* between the two samples (which becomes a *similarity function* when prepended with a negative sign), and $\\gamma$ is the \"cutoff parameter\" that defines how \"loose\" the fit of the boundary is. When $\\gamma$ is large, we'll get a \"tight\" fit and the hyperplane will closely hew to the training samples; when $\\gamma$ is small, we'll get a \"loose\" fit that will instead define rough regions in which the training samples reside.\n",
    "\n",
    "### Decision trees\n",
    "\n",
    "Up until this point, we've looked exclusively at learning algorithms that fit into the \"neural\" model established by the perceptron and Adaline algorithms. **Decision trees,** however, are a common family of classifiers that use a different learning architecture.\n",
    "\n",
    "On an intuitive level, decision trees define a set of *logical operations* to perform in stepwise fashion, with the goal of dividing the sample space into rule-based classification regions. For example, when learning on the famous Iris dataset, a decision tree might \"ask questions\" (i.e. perform boolean functions) to procedurally partition the sample space, like:\n",
    "\n",
    "- Is the petal length $\\ge$ 2.5cm?\n",
    "- Is the sepal length $\\le$ 5cm?\n",
    "\n",
    "Hence, the algorithm defines a \"tree\" of logic that acts as a function to return class labels.\n",
    "\n",
    "#### Maximizing information gain\n",
    "\n",
    "Up until now, most objective functions we've looked at have been some form of cost (or error) functions, with a basic goal that we could express in prose as \"determine how 'wrong' the model is based on one set of criteria, and then try to minimize that wrongness.\" Decision trees, on the other hand, introduce a radically new family of objective functions that aim to **maximize the information gain** of sample space partitions. \n",
    "\n",
    "Raschka defines the information gain of a partition like so:\n",
    "\n",
    "$$ IG(D_{p}, f) := I(D_{p}) - \\sum_{j=1}^{m} \\frac{N_{j}}{N_{p}} I(D_{j}) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $D_{p}$ is the **parent node** (the initial state of a region of the dataset, up to and including the sample space itself, that is set to be partitioned)\n",
    "- $f$ is the **partition** under consideration that splits up the parent node\n",
    "- $D_{j}$ is the **$j$th child node**\n",
    "- $I$ is the **impurity measure** that we use to quantify how much information is contained in a partition\n",
    "- $N$ is the **number of samples** in either the parent or child nodes\n",
    "\n",
    "In prose, we measure the information gain by finding the *difference between the impurity of the parent and the cumulative impurity of the child nodes*. If the child nodes are cumulatively less impure (i.e. more pure) than the parent node, we've gained information (and vice versa).\n",
    "\n",
    "For simplicity, we'll often use **binary partitions (trees)** of the parent nodes:\n",
    "\n",
    "$$ IG(D_{p}, f) = I(D_{p}) - \\bigg [ \\frac{N_{left}}{N_{p}} (D_{left}) + \\frac{N_{right}}{N_{p}} (D_{right}) \\bigg ] $$\n",
    "\n",
    "This means that each fork has at most two paths, and is computationally convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
