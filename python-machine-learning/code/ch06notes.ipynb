{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Chapter 6: Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "So far, Raschka has introduced us to an world of different models for classifying data and compressing features. But how do we know how well any given model is performing, and how can we figure out how to improve performance when it's bad?\n",
    "\n",
    "In Chapter 6, we explore many different techniques for improving model performance. Broadly, our focus includes:\n",
    "\n",
    "- Estimating model performance\n",
    "- Diagnosing common problems\n",
    "- Fine-tuning models by adjusting hyperparameters\n",
    "- Getting familiar with different performance metrics\n",
    "\n",
    "The specific techniques we'll cover are:\n",
    "\n",
    "1. [Data-processing pipelines](#Data-processing-pipelines): chaining algorithms together\n",
    "2. Cross-validation: robust measures of performance\n",
    "3. Learning and validation curves: measuring bias and variance\n",
    "4. Grid search: tuning hyperparameters\n",
    "5. Nested cross-validation: selecting good algorithms\n",
    "6. Performance metrics: different ways of judging \"good\" and \"bad\" models\n",
    "\n",
    "## Data-processing pipelines\n",
    "\n",
    "Many preprocessing techniques (like PCA) find parameters that must be reused across all training and testing datasets in order to produce sensible results. To help standardize this procedure, we can build **pipelines** that record our transformation steps and allow us to reuse them across training, testing, and validation sets (or even on new samples from the same population). Pipelines also encourage an object-oriented approach to model-building.\n",
    "\n",
    "In scikit-learn, we can use the [`Pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class to construct a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('clf', LogisticRegression)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `Pipeline` estimators have the standard set of `fit()`, `predict()`, and `fit_transform()` methods, but that in order to evaluate correctly, all steps up until the final classification step must call `fit_transform`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
