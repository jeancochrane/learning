{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Chapter 4\n",
    "\n",
    "### Imputing missing data\n",
    "\n",
    "Load the [Pima diabetes dataset]('http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data') as a pandas dataframe. (Note that the data does not include a header row. You'll have to build that yourself based on the documentation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     num_times_pregnant  glucose_concentration  blood_pressure  \\\n",
      "0                     6                    148              72   \n",
      "1                     1                     85              66   \n",
      "2                     8                    183              64   \n",
      "3                     1                     89              66   \n",
      "4                     0                    137              40   \n",
      "5                     5                    116              74   \n",
      "6                     3                     78              50   \n",
      "7                    10                    115               0   \n",
      "8                     2                    197              70   \n",
      "9                     8                    125              96   \n",
      "10                    4                    110              92   \n",
      "11                   10                    168              74   \n",
      "12                   10                    139              80   \n",
      "13                    1                    189              60   \n",
      "14                    5                    166              72   \n",
      "15                    7                    100               0   \n",
      "16                    0                    118              84   \n",
      "17                    7                    107              74   \n",
      "18                    1                    103              30   \n",
      "19                    1                    115              70   \n",
      "20                    3                    126              88   \n",
      "21                    8                     99              84   \n",
      "22                    7                    196              90   \n",
      "23                    9                    119              80   \n",
      "24                   11                    143              94   \n",
      "25                   10                    125              70   \n",
      "26                    7                    147              76   \n",
      "27                    1                     97              66   \n",
      "28                   13                    145              82   \n",
      "29                    5                    117              92   \n",
      "..                  ...                    ...             ...   \n",
      "738                   2                     99              60   \n",
      "739                   1                    102              74   \n",
      "740                  11                    120              80   \n",
      "741                   3                    102              44   \n",
      "742                   1                    109              58   \n",
      "743                   9                    140              94   \n",
      "744                  13                    153              88   \n",
      "745                  12                    100              84   \n",
      "746                   1                    147              94   \n",
      "747                   1                     81              74   \n",
      "748                   3                    187              70   \n",
      "749                   6                    162              62   \n",
      "750                   4                    136              70   \n",
      "751                   1                    121              78   \n",
      "752                   3                    108              62   \n",
      "753                   0                    181              88   \n",
      "754                   8                    154              78   \n",
      "755                   1                    128              88   \n",
      "756                   7                    137              90   \n",
      "757                   0                    123              72   \n",
      "758                   1                    106              76   \n",
      "759                   6                    190              92   \n",
      "760                   2                     88              58   \n",
      "761                   9                    170              74   \n",
      "762                   9                     89              62   \n",
      "763                  10                    101              76   \n",
      "764                   2                    122              70   \n",
      "765                   5                    121              72   \n",
      "766                   1                    126              60   \n",
      "767                   1                     93              70   \n",
      "\n",
      "     skin_fold_thickness  insulin   bmi  diabetes_pedigree  age  target  \n",
      "0                     35        0  33.6              0.627   50       1  \n",
      "1                     29        0  26.6              0.351   31       0  \n",
      "2                      0        0  23.3              0.672   32       1  \n",
      "3                     23       94  28.1              0.167   21       0  \n",
      "4                     35      168  43.1              2.288   33       1  \n",
      "5                      0        0  25.6              0.201   30       0  \n",
      "6                     32       88  31.0              0.248   26       1  \n",
      "7                      0        0  35.3              0.134   29       0  \n",
      "8                     45      543  30.5              0.158   53       1  \n",
      "9                      0        0   0.0              0.232   54       1  \n",
      "10                     0        0  37.6              0.191   30       0  \n",
      "11                     0        0  38.0              0.537   34       1  \n",
      "12                     0        0  27.1              1.441   57       0  \n",
      "13                    23      846  30.1              0.398   59       1  \n",
      "14                    19      175  25.8              0.587   51       1  \n",
      "15                     0        0  30.0              0.484   32       1  \n",
      "16                    47      230  45.8              0.551   31       1  \n",
      "17                     0        0  29.6              0.254   31       1  \n",
      "18                    38       83  43.3              0.183   33       0  \n",
      "19                    30       96  34.6              0.529   32       1  \n",
      "20                    41      235  39.3              0.704   27       0  \n",
      "21                     0        0  35.4              0.388   50       0  \n",
      "22                     0        0  39.8              0.451   41       1  \n",
      "23                    35        0  29.0              0.263   29       1  \n",
      "24                    33      146  36.6              0.254   51       1  \n",
      "25                    26      115  31.1              0.205   41       1  \n",
      "26                     0        0  39.4              0.257   43       1  \n",
      "27                    15      140  23.2              0.487   22       0  \n",
      "28                    19      110  22.2              0.245   57       0  \n",
      "29                     0        0  34.1              0.337   38       0  \n",
      "..                   ...      ...   ...                ...  ...     ...  \n",
      "738                   17      160  36.6              0.453   21       0  \n",
      "739                    0        0  39.5              0.293   42       1  \n",
      "740                   37      150  42.3              0.785   48       1  \n",
      "741                   20       94  30.8              0.400   26       0  \n",
      "742                   18      116  28.5              0.219   22       0  \n",
      "743                    0        0  32.7              0.734   45       1  \n",
      "744                   37      140  40.6              1.174   39       0  \n",
      "745                   33      105  30.0              0.488   46       0  \n",
      "746                   41        0  49.3              0.358   27       1  \n",
      "747                   41       57  46.3              1.096   32       0  \n",
      "748                   22      200  36.4              0.408   36       1  \n",
      "749                    0        0  24.3              0.178   50       1  \n",
      "750                    0        0  31.2              1.182   22       1  \n",
      "751                   39       74  39.0              0.261   28       0  \n",
      "752                   24        0  26.0              0.223   25       0  \n",
      "753                   44      510  43.3              0.222   26       1  \n",
      "754                   32        0  32.4              0.443   45       1  \n",
      "755                   39      110  36.5              1.057   37       1  \n",
      "756                   41        0  32.0              0.391   39       0  \n",
      "757                    0        0  36.3              0.258   52       1  \n",
      "758                    0        0  37.5              0.197   26       0  \n",
      "759                    0        0  35.5              0.278   66       1  \n",
      "760                   26       16  28.4              0.766   22       0  \n",
      "761                   31        0  44.0              0.403   43       1  \n",
      "762                    0        0  22.5              0.142   33       0  \n",
      "763                   48      180  32.9              0.171   63       0  \n",
      "764                   27        0  36.8              0.340   27       0  \n",
      "765                   23      112  26.2              0.245   30       0  \n",
      "766                    0        0  30.1              0.349   47       1  \n",
      "767                   31        0  30.4              0.315   23       0  \n",
      "\n",
      "[768 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "names = ['num_times_pregnant', 'glucose_concentration',\n",
    "          'blood_pressure', 'skin_fold_thickness', 'insulin',\n",
    "          'bmi', 'diabetes_pedigree', 'age', 'target']\n",
    "\n",
    "data_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/' +\\\n",
    "           'pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "\n",
    "df = pandas.read_csv(data_url, header=None, index_col=False, names=names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataframe to see which columns contain 0's. Based on the data type of each column, do these 0's all make sense? Which 0's are suspicious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_times_pregnant : True\n",
      "glucose_concentration : True\n",
      "blood_pressure : True\n",
      "skin_fold_thickness : True\n",
      "insulin : True\n",
      "bmi : True\n",
      "diabetes_pedigree : False\n",
      "age : False\n",
      "target : True\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print(name, ':', any(df.loc[:, name] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Columns 2-6 (glucose, blood pressure, skin fold thickness, insulin, and BMI) all contain zeros, but none of these measurements should ever be 0 in a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that 0s indiciate missing values, and fix them in the dataset by eliminating samples with missing features. Then run a logistic regression, and measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7384615384615385"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(1,6):\n",
    "    df.loc[df.loc[:, names[i]] == 0, names[i]] = np.nan\n",
    "\n",
    "df_no_nan = df.dropna(axis=0, how='any')\n",
    "\n",
    "X = df_no_nan.iloc[:, :8].values\n",
    "y = df_no_nan.iloc[:, 8].values\n",
    "\n",
    "def fit_and_score_rlr(X, y, normalize=True):\n",
    "    \n",
    "    if normalize:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X_std = scaler.transform(X)\n",
    "    else:\n",
    "        X_std = X\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_std, y,\n",
    "                                                        test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    rlr = LogisticRegression(C=1)\n",
    "\n",
    "    rlr.fit(X_train, y_train)\n",
    "    return rlr.score(X_test, y_test)\n",
    "\n",
    "fit_and_score_rlr(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, replace missing features through mean imputation. Run  a regression and measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76377952755905509"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "X = imputer.fit_transform(df.iloc[:, :8].values)\n",
    "\n",
    "y = df.iloc[:, 8].values\n",
    "\n",
    "fit_and_score_rlr(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Interestingly, there's not a huge performance improvement between the two approaches! In my run, using mean imputation corresponded to about a 3 point increase in model performance. Some ideas for why this might be:\n",
    "\n",
    "1. This is a small dataset to start out with, so removing ~half its samples doesn't change performance very much\n",
    "2. There's not much information contained in the features with missing data\n",
    "3. There are other effects underlying poor performance of the model (e.g. regularization parameters) that are having a greater impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing categorical variables\n",
    "\n",
    "Load the [TA evaluation dataset](https://archive.ics.uci.edu/ml/datasets/Teaching+Assistant+Evaluation). As before, the data and header are split into two files, so you'll have to combine them yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     native_speaker  instructor  course  season  class_size  rating\n",
      "0                 1          23       3       1          19       3\n",
      "1                 2          15       3       1          17       3\n",
      "2                 1          23       3       2          49       3\n",
      "3                 1           5       2       2          33       3\n",
      "4                 2           7      11       2          55       3\n",
      "5                 2          23       3       1          20       3\n",
      "6                 2           9       5       2          19       3\n",
      "7                 2          10       3       2          27       3\n",
      "8                 1          22       3       1          58       3\n",
      "9                 2          15       3       1          20       3\n",
      "10                2          10      22       2           9       3\n",
      "11                2          13       1       2          30       3\n",
      "12                2          18      21       2          29       3\n",
      "13                2           6      17       2          39       3\n",
      "14                2           6      17       2          42       2\n",
      "15                2           6      17       2          43       2\n",
      "16                2           7      11       2          10       2\n",
      "17                2          22       3       2          46       2\n",
      "18                2          13       3       1          10       2\n",
      "19                2           7      25       2          42       2\n",
      "20                2          25       7       2          27       2\n",
      "21                2          25       7       2          23       2\n",
      "22                2           2       9       2          31       2\n",
      "23                2           1      15       1          22       2\n",
      "24                2          15      13       2          37       2\n",
      "25                2           7      11       2          13       2\n",
      "26                2           8       3       2          24       2\n",
      "27                2          14      15       2          38       2\n",
      "28                2          21       2       2          42       1\n",
      "29                2          22       3       2          28       1\n",
      "..              ...         ...     ...     ...         ...     ...\n",
      "121               2          13      14       2          17       3\n",
      "122               2           9       6       2           7       3\n",
      "123               1          10       3       2          21       3\n",
      "124               2          14      15       2          36       3\n",
      "125               1          13       1       2          54       3\n",
      "126               1           8       3       2          29       3\n",
      "127               2          20       2       2          45       3\n",
      "128               2          22       1       2          11       2\n",
      "129               2          18      12       2          16       2\n",
      "130               2          20      15       2          18       2\n",
      "131               1          17      18       2          44       2\n",
      "132               2          14      23       2          17       2\n",
      "133               2          24      26       2          21       2\n",
      "134               2           9      24       2          20       2\n",
      "135               2          12       8       2          24       2\n",
      "136               2           9       6       2           5       2\n",
      "137               2          22       1       2          42       2\n",
      "138               2           7      11       2          30       1\n",
      "139               2          10       3       2          19       1\n",
      "140               2          23       3       2          11       1\n",
      "141               2          17      18       2          29       1\n",
      "142               2          16      20       2          15       1\n",
      "143               2           3       2       2          37       1\n",
      "144               2          19       4       2          10       1\n",
      "145               2          23       3       2          24       1\n",
      "146               2           3       2       2          26       1\n",
      "147               2          10       3       2          12       1\n",
      "148               1          18       7       2          48       1\n",
      "149               2          22       1       2          51       1\n",
      "150               2           2      10       2          27       1\n",
      "\n",
      "[151 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data'\n",
    "\n",
    "names = ['native_speaker', 'instructor', 'course', 'season', 'class_size', 'rating']\n",
    "\n",
    "df = pandas.read_csv(data_url, header=None, index_col=False, names=names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the features are categorical? Are they ordinal, or nominal? Which features are numeric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: According to the documentation:\n",
    "\n",
    "1. **Native speaker**: categorical (nominal)\n",
    "2. **Instructor**: categorical (nominal)\n",
    "3. **Course**: categorical (nominal)\n",
    "4. **Season**: categorical (nominal)\n",
    "5. **Class size**: numeric\n",
    "6. **Rating**: categorical (ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical variables in a naive fashion, by leaving them in place as numerics. Run a classification and measure performance against a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean/.virtualenvs/learning/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54000000000000004"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "fit_and_score_rlr(X, y, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, encode the categorical variables with a one-hot encoder. Again, run a classification and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56000000000000005"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(categorical_features=range(5))\n",
    "X_encoded = enc.fit_transform(X)\n",
    "\n",
    "fit_and_score_rlr(X_encoded, y, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "Raschka mentions that decision trees and random forests do not require standardized features prior to classification, while the rest of the classifiers we've seen so far do. Why might that be? Explain the intuition behind this idea based on the differences between tree-based classifiers and the other classifiers we've seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll test the two scaling algorithms on the wine dataset. Start by loading the [wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features via \"standardization\" (as Raschka describes it). Classify and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features via \"normalization\" (as Raschka describes it). Again, classify and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "- Implement SBS below. Then, run the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SBS(object):\n",
    "    \"\"\"\n",
    "    Class to select the k-best features in a dataset via sequential backwards selection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the SBS model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fit SBS to a dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform a dataset based on the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Fit SBS to a dataset and transform it, returning the k-best features.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll practice feature selection. Start by loading the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a random forest to determine the feature importances. Plot the features and their importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use L1 regularization with a standard C value (0.1) to eliminate low-information features. Again, plot the feature importances using the `coef_` attribute of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the feature importances from the random forest/L1 regularization compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
