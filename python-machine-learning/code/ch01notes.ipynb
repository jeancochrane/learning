{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Chapter 2: Training Machine Learning Algorithms for Classification\n",
    "\n",
    "### Background\n",
    "\n",
    "The first algorithms to be developed for classification tasks were the **perceptron** and **adaptive linear neurons**. These algorithms are the part of the early history of machine intelligence.\n",
    "\n",
    "The idea of **artificial neurons** was (according to Raschka) first developed by Warren McCulloch and Walter Pitts in 1943 – hence the name McCulloch-Pitts (MCP) neuron. McCulloch and Pitts were inspired by developments in cognitive biology (*early evidence of biology's status as an inspiration to ML -JC*).\n",
    "\n",
    "The intuition behind the MCP neuron is that a computational classifier could have a similar structure to a (*vastly oversimplified*) neuron: \n",
    "\n",
    "```\n",
    "Input -\\                              1\n",
    "Input --\\                            /\n",
    "Input -----> function --> ?threshold?\n",
    "Input --/                            \\ \n",
    "Input -/                              0\n",
    "```\n",
    "\n",
    "In 1957, Frank Rosenblatt extended this idea to what he called the **perceptron**, a neuron that would *learn* the optimal weights to apply to its input \"features\".\n",
    "\n",
    "### Rosenblatt's perceptron function\n",
    "\n",
    "Formally, Rosenblatt's perceptron defines a weight vector $w$ and an input vector $x$:\n",
    "\n",
    "$$\n",
    "w = \\left( \\begin{array}{c} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{m} \\end{array} \\right), \\quad \n",
    "x = \\left( \\begin{array}{c} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{m} \\end{array} \\right) \n",
    "$$\n",
    "  \n",
    "With an activation function:\n",
    "\n",
    "$$\\phi(z) = \\bigg\\{ \\begin{array}{rr} 1 & if \\> z\\ge\\theta \\\\ -1 & otherwise \\end{array}$$\n",
    "\n",
    "Where $z$ is a linear combination (dot product) of the weight vector and the input vector:\n",
    "\n",
    "$$z \\; = \\; w \\cdot x \\; = \\; w_{1}x_{1} + w_{2}x_{2} + \\; ... \\; + w_{m}x_{m}$$\n",
    "\n",
    "Hence, we can move $\\theta$ to the lefthand side:\n",
    "\n",
    "$$z \\ge \\theta \\quad \\Leftrightarrow \\quad -\\theta + z \\ge 0$$\n",
    "\n",
    "Include it as part of the inputs:\n",
    "\n",
    "$$\n",
    "w = \\left( \\begin{array}{c} -\\theta \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{m} \\end{array} \\right), \\quad\n",
    "x = \\left( \\begin{array}{c} 1 \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{m} \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "And redefine the activation function in terms of $0$:\n",
    "\n",
    "$$\\phi(z) = \\bigg\\{ \\begin{array}{rr} 1 & if z \\ge 0 \\\\ -1 & otherwise \\end{array}$$\n",
    "\n",
    "### Updating the perceptron weights\n",
    "\n",
    "The algorithm for updating the Rosenblatt perceptron weights is quite simple:\n",
    "\n",
    "1. Set the weights to 0, or small random numbers\n",
    "\n",
    "2. For each training vector $x^{(i)}$ (and its labeled output $\\hat{y^{(i)}}$):\n",
    "\n",
    "    1. Compute the output class label $\\hat{y^{(i)}}$\n",
    "    \n",
    "    2. Update the weights based on the error, $y^{(i)} - \\hat{y^{(i)}}$\n",
    "    \n",
    "Easy! So how do we update the weights? To do this, we'll sum the initial weights with an adjustment term $\\Delta w_{j}$:\n",
    "\n",
    "$$w_{j} = w_{j} + \\Delta w_{j}$$\n",
    "\n",
    "We can compute $\\Delta w_{j}$ like so:\n",
    "\n",
    "$$\\Delta x_{j} = \\eta \\; (y^{(i)} - \\hat{y^{(i)}}) \\; x^{(i)}_{j}$$\n",
    "\n",
    "Where $\\eta$ is a constant **learning rate** coefficient on the interval $[0, 1]$, $y_{i} - \\hat{y_{i}}$ is the **error term**, and $x^{(i)}_{j}$ is the element of the input vector at index $j$. Note that the coefficient `learning_rate * error` of the update value is constant for *all values* of the input vector $x$, but it is scaled according to the value of $x$ at index $j$, making it proportional to that value. This has the really nice property of adjusting the weights for larger values of $x_{j}$ more dramatically than for smaller values.\n",
    "\n",
    "Note that the perceptron algorithm assumes our classes are **linearly separable** – that is, we can find some linear combination that cleanly divides the classes in two.\n",
    "\n",
    "If the classes aren't linearly separable, we have two options:\n",
    "\n",
    "1. Define a finite number of iterations (**epoch**) after which we'll stop updating the weights\n",
    "2. Set a **threshold** of acceptable misclassification\n",
    "\n",
    "These approaches aren't mutually exclusive, of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
