{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Chapter 1\n",
    "\n",
    "### Training Machine Learning Algorithms for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** In the file `algos/perceptron.py`, implement Rosenblatt's perceptron algorithm by fleshing out the class `Perceptron`. When you're finished, run the code in the block below to test your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from algos.perceptron import Perceptron\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 1, -1)\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "ppn = Perceptron()\n",
    "ppn.fit(X, y)\n",
    "\n",
    "if (ppn.errors[-1] == 0):\n",
    "    print('Looks good!')\n",
    "else:\n",
    "    print(\"Looks like your classifier didn't converge to 0 :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Raschka claims that without an epoch or a threshold of acceptable misclassification, the perceptron may not ever stop updating. Explain why this can happen, and give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer**: For the perceptron error to converge to 0, the algorithm must reach a point where it does not misclassify any of the training data. If the training data is not *completely* linearly separable, however, this convergence is impossible: there simply does not exist a linear combination that cleanly separates the data into two classes.\n",
    "\n",
    "In the *Iris* dataset example that Raschka gives, we can completely separate *setosa* from *versicolor* based on petal length and sepal length (the two variables we test in the code sample above), but in most real-world data, such clean linear separation is unrealistic, and there is usually slight overlap between the two classes, even if they are generally linearly separable. By setting an epoch (maximum number of iterations) or threshold (maximum number of misclassifications that we can accept), we can ensure that the algorithm will stop training even if pure linear separation is impossible in the data.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** The following diagram comes from Raschka's book. Try to answer the questions about it without looking back at the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classifier charts](https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is being depicted in the diagram on the left? How about the diagram on the right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:** The diagram on the left plots the activation function $\\phi(w^{T}x)$ against the net input function $w^{T}x$ in a perceptron, demonstrating the classification effect of the activation function: when the combination of the weights and inputs is greater than 0, the classifier returns 1 (a \"positive\" class label), and otherwise it returns -1 (a \"negative\" class label). \n",
    "\n",
    "The diagram on the right plots two features $X_{1}$ and $X_{2}$ from an input vector against one another, showing that two classes (the red circles and blue checks) are linearly separable by the line $\\phi(w^{T}x) = 0 $.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in words what the following symbols represent in the diagram on the **left**:\n",
    "\n",
    "1. The axes, $w^{T}x$ and $\\phi(w^{T}x)$\n",
    "2. The thick black line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. The $x$ axis, $w^{T}x$, represents the net input function of a perceptron (the linear combination of weight and input vectors). The $y$ axis, $\\phi(w^{T}x)$, represents the activation function (the function that classifies the net input and returns a positive or negative class label).\n",
    "2. The thick black line represents the range of possible values taken on by the activation function $\\phi(w^{T}x)$. When the net input input is greater than or equal to 0, the activation funtion returns 1; otherwise it returns -1. Since it is a step function, the vertical portion of the line (shown when $w^{T}x = 0$) does not actually represent possible values in the range of $\\phi(w^{T}x)$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in words what the following symbols represent in the diagram on the **right**:\n",
    "\n",
    "1. The red circles\n",
    "2. The blue pluses\n",
    "3. The axes, $X_{1}$ and $X_{2}$\n",
    "4. The vertical dashed line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. The red circles represent samples that are classified by a negative class label.\n",
    "2. The blue pluses represent samples that are classified by a positive class label.\n",
    "3. The axes $X_{1}$ and $X_{2}$ represent two features of each sample that we can use to determine their class.\n",
    "4. The vertical dashed line represents the decision boundary between the two classes.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True or False: In the diagram on the right, $X_{1} = \\phi(w^{T}x) = 0$. Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:** False. While the right half of this equality (that the decision surface can be modelled by $\\phi(w^{T}x) = 0$) is true, the left half (that the feature $X_{1}$ is equivalent to  $\\phi(w^{T}x)$, or to $0$) is not. While the figure makes it seem as if the dashed line corresponding to the decision surface is related to the feature $X_{1}$, they are in fact unrelated, and their graphical overlap is a coincidence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True or False: in the general relationship depicted by the diagram on the right ($X_{1}$ vs. $X_{2}$), the dashed line must always be vertical. Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:** False. As the line representing the decision surface for the classifier, any line that cleanly separates the data into the two classes (positive and negative) will work.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Plot $X$ and its standardized form $X'$ following the feature scaling algorithm that Raschka uses in the book. How does scaling the feature using the $t$-statistic change the sample distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeancochrane/.virtualenvs/learning/lib/python3.6/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEPCAYAAACqZsSmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f58f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], color='red', marker='o', label='X')\n",
    "plt.scatter(X_std[:, 0], X_std[:, 1], color='blue', marker='x', label=\"X'\")\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As we can see in the figure above, scaling $X$ using the $t$-statistic maintains the overall shape of the data while centering the axes around the origin and transforming the samples such that the axes correspond to the number of standard deviations that each sample is from the sample means $\\mu_{x}$ and $\\mu_{y}$ (as opposed to the unit measurements for length present in the original data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** In the file `algos/adaline.py`, implement the Adaline rule in the class `Adaline`. When you're finished, run the code in the block below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "from algos.adaline import Adaline\n",
    "\n",
    "ada = Adaline()\n",
    "ada.fit(X_std, y)\n",
    "\n",
    "if (ada.cost[-1] < 5):\n",
    "    print('Looks good!')\n",
    "else:\n",
    "    print(\"Looks like your classifier didn't find the minimum :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Implement stochastic gradient descent as an option for the `Adaline` class. Then, run the test code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "ada_sgd = Adaline(stochastic=True)\n",
    "ada_sgd.fit(X_std, y)\n",
    "\n",
    "if (ada_sgd.cost[1] < 1):\n",
    "    print('Looks good!')\n",
    "else:\n",
    "    print(\"Looks like your stochastic model isn't performing well enough :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.** Describe a situation in which you would choose to use batch gradient descent, a situation in which you would choose to use stochastic gradient descent, and a situation in which you would choose to use mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:** I would want to use batch gradient descent in a situation where my dataset was relatively small, but the need for precision in the final approximation is high. I would want to use mini-batch gradient descent when my dataset is quite large, and computation power is limited. I would want to use stochastic gradient descent if I wanted to stream my training data.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8.** Implement online learning as an option for the `Adaline` class. Then, run the test code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = df.iloc[100, [0, 2]]\n",
    "new_X = new_X - (np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "new_y = df.iloc[100, 4]\n",
    "new_y = np.where(new_y == 'Iris-setosa', 1, -1)\n",
    "\n",
    "ada_sgd.partial_fit(new_X, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9.** Raschka claims that stochastic gradient descent could result in \"cycles\" if the order in which the samples were read (and corresponding weights updated) wasn't randomized, or \"shuffled,\" before every iteration. Explain the intuition behind this idea, and describe what a \"cycle\" might look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "**Answer:** Since SGD updates the weights with respect to each sample, and calculates the gradient separately for any given sample, it is sensitive to outliers (that is, samples with unusually large or small feature values). Intuitively, we can imagine that an outlier sample positioned at an inopportune index – close to the point where the model would converge, say – could consistently cause a model updating via SGD to \"overshoot\" the global minimum. Randomizing the order of the inputs helps make this kind of overshooting less likely as the iterations progress. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10.** Verify that stochastic gradient descent improves the speed of convergence for Adaline in the case of the Iris dataset by plotting the errors against the iteration epoch in both cases. Then, briefly explain why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = Adaline(eta=0.01)\n",
    "ada_sgd = Adaline(eta=0.01, stochastic=True)\n",
    "\n",
    "ada.fit(X_std, y)\n",
    "ada_sgd.fit(X_std, y)\n",
    "\n",
    "plt.plot(range(1, len(ada.cost) + 1), ada.cost, color='red', marker='o', label='Standard')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(1, len(ada_sgd.cost) + 1), ada_sgd.cost, color='blue', marker='x', label='SGD')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** Since SGD calculates a new gradient (and, correspondingly, a new set of weights) every time it considers a new sample, it updates much faster than standard gradient descent.\n",
    "\n",
    "At the same time, it's important to note that the costs in the figures above are not completely comparable. For SGD, we compute *average* cost across the entire set of samples for each iteration, while for standard gradient descent we compute the *sum* of the costs, so the cost functions produce different units (average cost vs. total cost for a set of samples). Still, we can eyeball the speed of the convergence and confirm that SGD does, in fact, converge on a global minimum faster than standard gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
